{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# 2D to 3D Side-by-Side Video Converter (GPU Optimized)\n",
        "\n",
        "This notebook converts standard 2D videos into stereoscopic 3D Side-by-Side (SBS) format for VR viewing. It uses MiDaS for depth estimation and creates a stereoscopic effect by synthesizing left and right eye views.\n",
        "\n",
        "## Features\n",
        "- Optimized for maximum GPU utilization (works with any available GPU memory)\n",
        "- Video segment selection for processing specific portions of longer videos\n",
        "- Upload videos (up to 500MB) or provide video URLs\n",
        "- Adjustable depth parameters (intensity, convergence, eye separation)\n",
        "- High-quality H.264 encoded MP4 output in SBS format with 16:9 overall aspect ratio\n",
        "- **Preserves original audio track** in the output video\n",
        "- Real-time preview and parameter adjustment\n",
        "- Progress tracking and error handling\n",
        "\n",
        "## Setup\n",
        "Run the installation cell below to set up the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "installation",
        "outputId": "ae289a44-13d7-4912-a2c2-4d61dd3678c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install required packages\n",
        "!pip install opencv-python-headless\n",
        "!pip install numpy\n",
        "!pip install gradio\n",
        "!pip install torch torchvision\n",
        "!pip install timm\n",
        "!pip install yt-dlp\n",
        "!pip install pytube\n",
        "!pip install gdown\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "# Clone MiDaS repository and install its dependencies\n",
        "!git clone https://github.com/isl-org/MiDaS.git\n",
        "!pip install -q -r MiDaS/requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.6.30)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "fatal: destination path 'MiDaS' already exists and is not an empty directory.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'MiDaS/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import all necessary libraries for video processing, depth estimation, and the user interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "import_libs",
        "outputId": "eb5138be-7e42-4c2c-d4cd-b0c5a4b6391a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import urllib.request\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "import threading\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import subprocess\n",
        "import gdown\n",
        "from google.colab import files\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Memory: {gpu_mem:.2f} GB\")\n",
        "    # Set CUDA device to GPU 0\n",
        "    torch.cuda.set_device(0)\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "# Set default tensor type to cuda if available\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "# Optional: Set environment variable for PyTorch memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "midas_setup"
      },
      "source": [
        "## MiDaS Setup\n",
        "\n",
        "Initialize the MiDaS depth estimation model with GPU optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "setup_midas"
      },
      "source": [
        "# Audio processing functions\n",
        "def check_audio_stream(file_path):\n",
        "    \"\"\"Check if the video file has an audio stream\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-select_streams', 'a:0',\n",
        "             '-show_entries', 'stream=codec_name', '-of', 'default=noprint_wrappers=1:nokey=1',\n",
        "             file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        # If there's output, an audio stream was found\n",
        "        return bool(result.stdout.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking audio stream: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def extract_audio(input_path, output_path):\n",
        "    \"\"\"Extract audio from a video file using ffmpeg\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        # Use ffmpeg to extract audio\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-i', input_path,        # Input file\n",
        "            '-vn',                   # Disable video\n",
        "            '-acodec', 'copy',       # Copy audio codec without re-encoding\n",
        "            '-y',                    # Overwrite output file if it exists\n",
        "            output_path\n",
        "        ]\n",
        "\n",
        "        print(f\"Extracting audio from {input_path} to {output_path}...\")\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error extracting audio: {result.stderr}\")\n",
        "            return None\n",
        "\n",
        "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "            print(\"Audio extraction failed - output file is empty or missing\")\n",
        "            return None\n",
        "\n",
        "        print(\"Audio extracted successfully\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error during audio extraction: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def combine_video_audio(video_path, audio_path, output_path):\n",
        "    \"\"\"Combine video and audio files using ffmpeg\"\"\"\n",
        "    try:\n",
        "        # Use ffmpeg to merge video and audio\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-i', video_path,        # Video file\n",
        "            '-i', audio_path,        # Audio file\n",
        "            '-c:v', 'copy',          # Copy video without re-encoding\n",
        "            '-c:a', 'aac',           # Use AAC for audio (better compatibility)\n",
        "            '-b:a', '192k',          # Audio bitrate\n",
        "            '-shortest',             # Match the duration of the shorter file\n",
        "            '-y',                    # Overwrite output file if it exists\n",
        "            output_path\n",
        "        ]\n",
        "\n",
        "        print(f\"Combining video and audio into {output_path}...\")\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error combining video and audio: {result.stderr}\")\n",
        "            return False\n",
        "\n",
        "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "            print(\"Combination failed - output file is empty or missing\")\n",
        "            return False\n",
        "\n",
        "        print(\"Video and audio combined successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error during combination: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def setup_midas():\n",
        "    \"\"\"Initialize and return the MiDaS model for depth estimation using torch.hub\n",
        "    with optimizations for GPU usage\"\"\"\n",
        "    print(\"Loading MiDaS depth estimation model...\")\n",
        "\n",
        "    # Clean up any existing GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Select device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # Print GPU info\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
        "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
        "\n",
        "    # Load model - using DPT Large for best quality\n",
        "    try:\n",
        "        # Try to disable torch hub cache to ensure we get a fresh model\n",
        "        torch.hub.set_dir(tempfile.mkdtemp())\n",
        "        midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Fallback method\n",
        "        print(\"Trying alternate loading method...\")\n",
        "        midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\", trust_repo=True)\n",
        "\n",
        "    midas.to(device)\n",
        "    midas.eval()  # Set to evaluation mode\n",
        "\n",
        "    # If using CUDA, optimize model for inference\n",
        "    if device.type == 'cuda':\n",
        "        # Enable cuDNN benchmark mode for best performance with fixed input sizes\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        # We'll skip TorchScript optimization as it's causing issues\n",
        "        print(\"Skipping TorchScript optimization due to compatibility issues\")\n",
        "\n",
        "    # Load transforms\n",
        "    try:\n",
        "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transforms: {e}\")\n",
        "        # Fallback\n",
        "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
        "\n",
        "    transform = midas_transforms.dpt_transform\n",
        "\n",
        "    # Report GPU memory usage after model loading\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory After Model Load: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
        "\n",
        "    print(\"MiDaS model loaded successfully!\")\n",
        "    return midas, transform, device"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_processing"
      },
      "source": [
        "## Video Processing Functions\n",
        "\n",
        "Functions for video input validation, frame extraction, audio processing, and depth map generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "video_functions"
      },
      "source": [
        "def validate_video(file_path):\n",
        "    \"\"\"Validate if the input video file is supported\"\"\"\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        return False, \"File does not exist\"\n",
        "\n",
        "    # Check file extension\n",
        "    valid_extensions = [\".mp4\", \".avi\", \".mov\", \".webm\", \".mkv\"]\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "    if file_ext not in valid_extensions:\n",
        "        return False, f\"Unsupported file format: {file_ext}. Supported formats: {', '.join(valid_extensions)}\"\n",
        "\n",
        "    # Check if OpenCV can open the file\n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    if not cap.isOpened():\n",
        "        return False, \"Cannot open video file with OpenCV\"\n",
        "\n",
        "    # Check resolution\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    if width > 3840 or height > 2160:\n",
        "        cap.release()\n",
        "        return False, f\"Video resolution ({width}x{height}) exceeds maximum supported resolution (3840x2160)\"\n",
        "\n",
        "    # The file size check that was here has been removed.\n",
        "\n",
        "    # Get video info\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration_sec = frame_count / fps if fps > 0 else 0\n",
        "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024) # This line is now just for info\n",
        "    cap.release()\n",
        "\n",
        "    return True, {\"width\": width, \"height\": height, \"fps\": fps, \"frame_count\": frame_count,\n",
        "                  \"size_mb\": file_size_mb, \"duration_sec\": duration_sec}\n",
        "\n",
        "def extract_video_segment(input_path, output_path, start_time, end_time):\n",
        "    \"\"\"Extract a segment from a video file using ffmpeg\"\"\"\n",
        "    try:\n",
        "        # Ensure the output directory exists\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting segment from {start_time:.2f}s to {end_time:.2f}s...\")\n",
        "\n",
        "        # Use ffmpeg to extract the segment with stream copy\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-i', input_path,        # Input file\n",
        "            '-ss', str(start_time),  # Start time in seconds\n",
        "            '-to', str(end_time),    # End time in seconds\n",
        "            '-c:v', 'copy',          # Copy video stream without re-encoding\n",
        "            '-c:a', 'copy',          # Copy audio stream without re-encoding\n",
        "            '-avoid_negative_ts', '1',  # Avoid negative timestamps\n",
        "            '-y',                    # Overwrite output file if it exists\n",
        "            output_path\n",
        "        ]\n",
        "\n",
        "        # Run the command and capture output\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        # Check if the command was successful\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error extracting segment: {result.stderr}\")\n",
        "            raise Exception(f\"ffmpeg error: {result.stderr}\")\n",
        "\n",
        "        # Verify the output file exists and has content\n",
        "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "            raise ValueError(\"Segment extraction failed - output file is empty or missing\")\n",
        "\n",
        "        print(f\"Segment extracted successfully: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting video segment: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def ensure_h264_mp4(input_path, temp_dir=\"temp_videos\"):\n",
        "    \"\"\"Convert video to H.264 MP4 format if needed - optimized for speed\"\"\"\n",
        "    # Generate a new filename for the converted video\n",
        "    output_path = os.path.join(temp_dir, f\"h264_{int(time.time())}.mp4\")\n",
        "\n",
        "    # Use ffprobe to check if the video is already H.264 encoded\n",
        "    try:\n",
        "        print(f\"Checking encoding of {input_path}...\")\n",
        "        # Get video codec information with a short timeout\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
        "             '-show_entries', 'stream=codec_name', '-of', 'default=noprint_wrappers=1:nokey=1',\n",
        "             input_path],\n",
        "            capture_output=True, text=True, check=True, timeout=10\n",
        "        )\n",
        "        codec = result.stdout.strip()\n",
        "\n",
        "        if codec.lower() in ['h264', 'avc1']:\n",
        "            print(f\"Video is already H.264 encoded (codec: {codec})\")\n",
        "            return input_path\n",
        "        else:\n",
        "            print(f\"Video is not H.264 encoded (detected codec: {codec}). Converting with fast settings...\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"Codec detection timed out. Proceeding with conversion...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking video codec: {str(e)}. Converting with fast settings...\")\n",
        "\n",
        "    # Check if input file exists and has content\n",
        "    if not os.path.exists(input_path) or os.path.getsize(input_path) == 0:\n",
        "        raise ValueError(f\"Input file {input_path} does not exist or is empty\")\n",
        "\n",
        "    # Convert to H.264 MP4 with hardware acceleration if available\n",
        "    try:\n",
        "        print(\"Starting fast H.264 conversion...\")\n",
        "\n",
        "        # Try using hardware acceleration if available\n",
        "        # NVIDIA GPU acceleration\n",
        "        hw_accel_commands = [\n",
        "            # NVIDIA NVENC (if available)\n",
        "            [\n",
        "                'ffmpeg',\n",
        "                '-i', input_path,\n",
        "                '-c:v', 'h264_nvenc',  # NVIDIA GPU acceleration\n",
        "                '-preset', 'p1',  # Fast encoding preset\n",
        "                '-tune', 'hq',  # High quality tuning\n",
        "                '-rc:v', 'vbr',  # Variable bitrate\n",
        "                '-cq:v', '23',  # Quality level\n",
        "                '-b:v', '5M',  # Target bitrate\n",
        "                '-maxrate:v', '10M',  # Maximum bitrate\n",
        "                '-bufsize:v', '10M',  # Buffer size\n",
        "                '-c:a', 'aac',  # Audio codec\n",
        "                '-b:a', '128k',  # Audio bitrate\n",
        "                '-y',  # Overwrite output if exists\n",
        "                output_path\n",
        "            ],\n",
        "            # Fallback to CPU with ultrafast preset\n",
        "            [\n",
        "                'ffmpeg',\n",
        "                '-i', input_path,\n",
        "                '-c:v', 'libx264',  # CPU encoding\n",
        "                '-preset', 'ultrafast',  # Fastest encoding\n",
        "                '-tune', 'fastdecode',  # Fast decoding optimization\n",
        "                '-crf', '28',  # Lower quality for speed\n",
        "                '-g', '30',  # Keyframe every 30 frames\n",
        "                '-bf', '0',  # No B-frames (faster)\n",
        "                '-c:a', 'aac',  # Audio codec\n",
        "                '-b:a', '128k',  # Low audio bitrate\n",
        "                '-ac', '2',  # Stereo audio\n",
        "                '-ar', '44100',  # Standard audio sample rate\n",
        "                '-strict', 'experimental',\n",
        "                '-y',  # Overwrite output\n",
        "                output_path\n",
        "            ]\n",
        "        ]\n",
        "\n",
        "        # Try each acceleration method in order\n",
        "        success = False\n",
        "        for i, command in enumerate(hw_accel_commands):\n",
        "            try:\n",
        "                print(f\"Trying encoding method {i+1}...\")\n",
        "\n",
        "                # Run the command\n",
        "                print(f\"Running conversion command: {' '.join(command)}\")\n",
        "                process = subprocess.Popen(\n",
        "                    command,\n",
        "                    stdout=subprocess.PIPE,\n",
        "                    stderr=subprocess.PIPE,\n",
        "                    universal_newlines=True\n",
        "                )\n",
        "\n",
        "                # Set timeout for conversion (5 minutes)\n",
        "                timeout = 300  # seconds\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Monitor progress\n",
        "                while process.poll() is None:\n",
        "                    # Check if timeout has been reached\n",
        "                    if time.time() - start_time > timeout:\n",
        "                        process.terminate()\n",
        "                        raise TimeoutError(f\"Conversion timed out after {timeout} seconds\")\n",
        "\n",
        "                    # Print progress indicator\n",
        "                    print(\".\", end=\"\", flush=True)\n",
        "                    time.sleep(1)\n",
        "\n",
        "                # Check if successful\n",
        "                if process.returncode == 0 and os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                    print(f\"\\nSuccessfully converted to H.264 MP4 using method {i+1}: {output_path}\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"\\nMethod {i+1} failed with error code {process.returncode}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with method {i+1}: {str(e)}\")\n",
        "\n",
        "        if success:\n",
        "            return output_path\n",
        "        else:\n",
        "            # Fallback to simple copy method (no re-encoding)\n",
        "            try:\n",
        "                print(\"Attempting direct copy method as fallback...\")\n",
        "                subprocess.run([\n",
        "                    'ffmpeg',\n",
        "                    '-i', input_path,\n",
        "                    '-c', 'copy',  # Just copy streams without re-encoding\n",
        "                    '-y',\n",
        "                    output_path\n",
        "                ], check=True, timeout=300)\n",
        "\n",
        "                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                    print(f\"Successfully copied video to MP4 container: {output_path}\")\n",
        "                    return output_path\n",
        "                else:\n",
        "                    print(\"Copy method failed to produce a valid output file\")\n",
        "                    # If all conversion methods fail, return the original file path\n",
        "                    return input_path\n",
        "            except Exception as e2:\n",
        "                print(f\"All conversion methods failed: {str(e2)}\")\n",
        "                # If all conversion methods fail, return the original file path\n",
        "                return input_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during conversion: {str(e)}\")\n",
        "        # Return the original file path if all else fails\n",
        "        return input_path\n",
        "\n",
        "def get_video_duration(file_path):\n",
        "    \"\"\"Get the duration of a video file in seconds using ffprobe\"\"\"\n",
        "    try:\n",
        "        # Use ffprobe to get the duration\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n",
        "             '-of', 'default=noprint_wrappers=1:nokey=1', file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "            check=True\n",
        "        )\n",
        "        duration = float(result.stdout.strip())\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {str(e)}\")\n",
        "        # Fall back to OpenCV\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(file_path)\n",
        "            if not cap.isOpened():\n",
        "                return 0\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            duration = frame_count / fps if fps > 0 else 0\n",
        "            cap.release()\n",
        "            return duration\n",
        "        except Exception as e2:\n",
        "            print(f\"Error getting duration with OpenCV: {str(e2)}\")\n",
        "            return 0\n",
        "\n",
        "def download_from_url(url):\n",
        "    \"\"\"Download video from URL and return local file path\"\"\"\n",
        "    # Create temp directory if it doesn't exist\n",
        "    temp_dir = \"temp_videos\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Generate a temporary filename\n",
        "    timestamp = int(time.time())\n",
        "    temp_file = os.path.join(temp_dir, f\"downloaded_video_{timestamp}.mp4\")\n",
        "\n",
        "    try:\n",
        "        # Check if it's a Google Drive URL\n",
        "        if \"drive.google.com\" in url:\n",
        "            print(f\"Detected Google Drive URL. Downloading with gdown...\")\n",
        "            gdown.download(url, temp_file, quiet=False, fuzzy=True)\n",
        "\n",
        "            if not os.path.exists(temp_file) or os.path.getsize(temp_file) == 0:\n",
        "                raise Exception(\"gdown failed to download the file.\")\n",
        "\n",
        "            print(f\"Successfully downloaded from Google Drive to {temp_file}\")\n",
        "            # Ensure the downloaded file is in a compatible format\n",
        "            return ensure_h264_mp4(temp_file, temp_dir)\n",
        "\n",
        "        # Check if it's a YouTube URL (logic from your original code)\n",
        "        elif \"youtube.com/\" in url or \"youtu.be/\" in url:\n",
        "            print(f\"Attempting to download YouTube video from: {url}\")\n",
        "            # Using yt-dlp for YouTube downloads\n",
        "            !yt-dlp -f \"best[ext=mp4]/best\" -o \"{temp_file}\" \"{url}\"\n",
        "\n",
        "            if not os.path.exists(temp_file) or os.path.getsize(temp_file) == 0:\n",
        "                raise Exception(\"yt-dlp failed to download the video.\")\n",
        "\n",
        "            print(f\"Successfully downloaded video with 'best' format to {temp_file}\")\n",
        "            return ensure_h264_mp4(temp_file, temp_dir)\n",
        "\n",
        "        # Fallback for other direct URLs\n",
        "        else:\n",
        "            print(f\"Downloading from direct URL: {url}\")\n",
        "            urllib.request.urlretrieve(url, temp_file)\n",
        "            print(f\"Successfully downloaded to {temp_file}\")\n",
        "            return ensure_h264_mp4(temp_file, temp_dir)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during download: {str(e)}\")\n",
        "        # Check if a partial file was created and clean it up\n",
        "        if os.path.exists(temp_file):\n",
        "            os.remove(temp_file)\n",
        "        return None\n",
        "\n",
        "# Function to estimate depth for a single frame\n",
        "def estimate_depth(frame, model, transform, device):\n",
        "    \"\"\"Estimate depth for a single frame using MiDaS\"\"\"\n",
        "    # Preprocess image for MiDaS (using torch.hub transforms)\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    # Compute depth prediction\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_batch)\n",
        "        # Resize prediction to original frame size\n",
        "        prediction = torch.nn.functional.interpolate(\n",
        "            prediction.unsqueeze(1),\n",
        "            size=frame.shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False,\n",
        "        ).squeeze()\n",
        "\n",
        "    depth = prediction.cpu().numpy()\n",
        "\n",
        "    # Normalize depth map to 0-1 range\n",
        "    depth_min = depth.min()\n",
        "    depth_max = depth.max()\n",
        "    if depth_max - depth_min > 0:\n",
        "        depth = (depth - depth_min) / (depth_max - depth_min)\n",
        "    else:\n",
        "        depth = np.zeros(depth.shape, dtype=depth.dtype)\n",
        "\n",
        "    return depth\n",
        "\n",
        "# Process batches of frames efficiently\n",
        "def process_batch(frames, model, transform, device):\n",
        "    \"\"\"Process a batch of frames to get depth maps\"\"\"\n",
        "    depth_maps = []\n",
        "\n",
        "    # Process each frame in the batch separately\n",
        "    # This is more compatible than trying to batch process\n",
        "    for frame in frames:\n",
        "        depth_map = estimate_depth(frame, model, transform, device)\n",
        "        depth_maps.append(depth_map)\n",
        "\n",
        "    return depth_maps"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stereo_conversion"
      },
      "source": [
        "## Stereoscopic Conversion\n",
        "\n",
        "Functions to create stereoscopic side-by-side views from original frames and depth maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "stereo_functions"
      },
      "source": [
        "def create_depth_based_disparity(depth_map, depth_intensity, convergence, eye_separation):\n",
        "    \"\"\"Create disparity map from depth map using the control parameters\"\"\"\n",
        "    # Invert depth map since closer objects should have larger disparity\n",
        "    inverted_depth = 1.0 - depth_map\n",
        "\n",
        "    # Apply intensity control\n",
        "    disparity = inverted_depth * depth_intensity\n",
        "\n",
        "    # Apply eye separation and convergence adjustment\n",
        "    disparity = disparity * eye_separation / convergence\n",
        "\n",
        "    return disparity\n",
        "\n",
        "def generate_stereo_views(frame, depth_map, depth_intensity, convergence, eye_separation):\n",
        "    \"\"\"Generate left and right eye views for stereoscopic 3D\"\"\"\n",
        "    h, w = frame.shape[:2]\n",
        "\n",
        "    # Create disparity map from depth map\n",
        "    disparity = create_depth_based_disparity(depth_map, depth_intensity, convergence, eye_separation)\n",
        "\n",
        "    # Scale disparity to pixel displacement (max 5% of image width)\n",
        "    max_shift = int(w * 0.05)\n",
        "    disparity_scaled = disparity * max_shift\n",
        "\n",
        "    # Create empty images for left and right views\n",
        "    left_view = np.zeros_like(frame)\n",
        "    right_view = np.zeros_like(frame)\n",
        "\n",
        "    # For each row in the image\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            # Calculate shift for this pixel\n",
        "            shift = disparity_scaled[y, x]\n",
        "\n",
        "            # Calculate left and right positions\n",
        "            left_x = max(0, min(w-1, int(x - shift/2)))\n",
        "            right_x = max(0, min(w-1, int(x + shift/2)))\n",
        "\n",
        "            # Copy pixel values\n",
        "            left_view[y, left_x] = frame[y, x]\n",
        "            right_view[y, right_x] = frame[y, x]\n",
        "\n",
        "    # Fill holes using inpainting\n",
        "    # Create masks for unfilled areas\n",
        "    left_mask = np.all(left_view == 0, axis=2).astype(np.uint8) * 255\n",
        "    right_mask = np.all(right_view == 0, axis=2).astype(np.uint8) * 255\n",
        "\n",
        "    # Inpainting\n",
        "    if np.any(left_mask):\n",
        "        left_view = cv2.inpaint(left_view, left_mask, 3, cv2.INPAINT_TELEA)\n",
        "    if np.any(right_mask):\n",
        "        right_view = cv2.inpaint(right_view, right_mask, 3, cv2.INPAINT_TELEA)\n",
        "\n",
        "    return left_view, right_view\n",
        "\n",
        "def create_side_by_side(left_view, right_view):\n",
        "    \"\"\"Combine left and right views into a side-by-side 3D format with 16:9 overall aspect ratio\n",
        "    and 4:3 aspect ratio for each eye. Each eye view is embedded in a 16:9 frame with black bars.\"\"\"\n",
        "\n",
        "    # Fixed dimensions for the final 16:9 output\n",
        "    total_width = 1920   # Total width for 16:9 aspect ratio\n",
        "    total_height = 1080  # Total height for 16:9 aspect ratio\n",
        "\n",
        "    # Each eye gets half the width\n",
        "    eye_width = total_width // 2  # 960px per eye\n",
        "\n",
        "    # Determine content height for 4:3 aspect ratio within each eye view\n",
        "    content_height = int(eye_width * 3/4)  # 720px for 4:3 ratio at 960px width\n",
        "\n",
        "    # Resize views to exact 4:3 dimensions for each eye\n",
        "    left_resized = cv2.resize(left_view, (eye_width, content_height))\n",
        "    right_resized = cv2.resize(right_view, (eye_width, content_height))\n",
        "\n",
        "    # Create a black canvas with 16:9 aspect ratio\n",
        "    sbs_frame = np.zeros((total_height, total_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # Calculate vertical offset to center content (black bars at top and bottom)\n",
        "    vertical_offset = (total_height - content_height) // 2\n",
        "\n",
        "    # Place the views side by side in the center of the frame with black bars\n",
        "    sbs_frame[vertical_offset:vertical_offset+content_height, 0:eye_width] = left_resized\n",
        "    sbs_frame[vertical_offset:vertical_offset+content_height, eye_width:total_width] = right_resized\n",
        "\n",
        "    return sbs_frame"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "processing_function"
      },
      "source": [
        "## Main Video Processing Function\n",
        "\n",
        "Implements the core processing pipeline that converts the 2D video to 3D SBS format with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "main_processing"
      },
      "source": [
        "def process_video_to_3d_sbs(input_path, output_path, depth_intensity, convergence, eye_separation,\n",
        "                           progress=None, use_segment=False, segment_start=0, segment_end=None):\n",
        "    \"\"\"Convert a 2D video to 3D SBS using MiDaS depth estimation with GPU optimization\"\"\"\n",
        "    try:\n",
        "        # Validate input video\n",
        "        valid, result = validate_video(input_path)\n",
        "        if not valid:\n",
        "            raise ValueError(result)\n",
        "\n",
        "        video_info = result\n",
        "\n",
        "        # Create temporary directory for intermediate files\n",
        "        temp_dir = \"temp_videos\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Base names for temp files\n",
        "        timestamp = int(time.time())\n",
        "        temp_base = os.path.join(temp_dir, f\"temp_{timestamp}\")\n",
        "        temp_video_path = f\"{temp_base}_video.mp4\"  # For video without audio\n",
        "        temp_audio_path = f\"{temp_base}_audio.aac\"  # For extracted audio\n",
        "\n",
        "        # Track whether we're processing a segment\n",
        "        is_segment = False\n",
        "        original_input = input_path\n",
        "\n",
        "        # Extract audio from the source video (original or segment)\n",
        "        has_audio = check_audio_stream(input_path)\n",
        "        if has_audio:\n",
        "            print(\"Detected audio stream in the video\")\n",
        "            if extract_audio(input_path, temp_audio_path):\n",
        "                print(f\"Audio extracted to {temp_audio_path}\")\n",
        "            else:\n",
        "                print(\"Could not extract audio. Output will have no sound.\")\n",
        "                has_audio = False\n",
        "        else:\n",
        "            print(\"No audio stream detected in the video\")\n",
        "\n",
        "        # If using a segment, extract it first\n",
        "        segment_path = None\n",
        "        if use_segment and segment_start is not None and segment_end is not None and segment_start < segment_end:\n",
        "            try:\n",
        "                # Create temporary segment file\n",
        "                temp_dir = \"temp_videos\"\n",
        "                os.makedirs(temp_dir, exist_ok=True)\n",
        "                segment_path = os.path.join(temp_dir, f\"segment_{int(time.time())}.mp4\")\n",
        "\n",
        "                # Extract the segment\n",
        "                segment_path = extract_video_segment(input_path, segment_path, segment_start, segment_end)\n",
        "\n",
        "                # Use the segment file for processing\n",
        "                input_path = segment_path\n",
        "\n",
        "                # Re-validate the segment\n",
        "                valid, result = validate_video(input_path)\n",
        "                if not valid:\n",
        "                    raise ValueError(f\"Segment validation failed: {result}\")\n",
        "\n",
        "                video_info = result\n",
        "                print(f\"Using video segment from {segment_start}s to {segment_end}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting segment: {str(e)}. Processing entire video instead.\")\n",
        "                # Continue with the original file if segment extraction fails\n",
        "\n",
        "        width, height = video_info[\"width\"], video_info[\"height\"]\n",
        "        fps = video_info[\"fps\"]\n",
        "        frame_count = int(video_info[\"frame_count\"])\n",
        "\n",
        "        # Clear GPU memory before starting\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Setup MiDaS model - note: only getting 3 return values now\n",
        "        model, transform, device = setup_midas()\n",
        "\n",
        "        # Determine optimal batch size based on available GPU memory and resolution\n",
        "        batch_size = 1  # Default\n",
        "        if torch.cuda.is_available():\n",
        "            # Calculate available memory\n",
        "            available_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "            current_mem = torch.cuda.memory_allocated()\n",
        "            free_mem = available_mem - current_mem\n",
        "\n",
        "            # Heuristic for batch size based on resolution\n",
        "            pixel_count = width * height\n",
        "            if pixel_count <= 640 * 480:  # SD video\n",
        "                batch_size = 8\n",
        "            elif pixel_count <= 1280 * 720:  # HD video\n",
        "                batch_size = 4\n",
        "            elif pixel_count <= 1920 * 1080:  # Full HD\n",
        "                batch_size = 2\n",
        "            else:  # 4K\n",
        "                batch_size = 1\n",
        "\n",
        "            print(f\"Using batch size: {batch_size} for {width}x{height} video\")\n",
        "            print(f\"Available GPU memory: {free_mem/1e9:.2f}GB\")\n",
        "\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "        # Create output video writer\n",
        "        target_width, target_height = 1920, 1080  # 16:9 overall aspect ratio\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # H.264 codec\n",
        "        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (target_width, target_height))\n",
        "\n",
        "        # Process frames\n",
        "        frame_index = 0\n",
        "        prev_depth_map = None\n",
        "\n",
        "        # Report initial memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
        "\n",
        "        # Process video in batches\n",
        "        while True:\n",
        "            # Read batch of frames\n",
        "            frames = []\n",
        "            for _ in range(batch_size):\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frames.append(frame)\n",
        "\n",
        "            if not frames:\n",
        "                break  # End of video\n",
        "\n",
        "            # Process batch of frames to get depth maps\n",
        "            depth_maps = process_batch(frames, model, transform, device)\n",
        "\n",
        "            # Process each frame with its depth map\n",
        "            for i in range(len(frames)):\n",
        "                frame = frames[i]\n",
        "                depth_map = depth_maps[i]\n",
        "\n",
        "                # Apply temporal smoothing\n",
        "                if prev_depth_map is not None:\n",
        "                    depth_map = 0.8 * depth_map + 0.2 * prev_depth_map\n",
        "                prev_depth_map = depth_map.copy()\n",
        "\n",
        "                # Generate stereo views\n",
        "                left_view, right_view = generate_stereo_views(frame, depth_map, depth_intensity, convergence, eye_separation)\n",
        "\n",
        "                # Create side-by-side frame\n",
        "                sbs_frame = create_side_by_side(left_view, right_view)\n",
        "\n",
        "                # Write frame to output\n",
        "                out.write(sbs_frame)\n",
        "\n",
        "                # Update progress\n",
        "                frame_index += 1\n",
        "                if progress is not None:\n",
        "                    progress(min(1.0, frame_index / frame_count))\n",
        "\n",
        "                # Report GPU memory periodically\n",
        "                if frame_index % 10 == 0 and torch.cuda.is_available():\n",
        "                    memory_used_gb = torch.cuda.memory_allocated(0)/1e9\n",
        "                    total_mem_gb = torch.cuda.get_device_properties(0).total_memory/1e9\n",
        "                    usage_percent = (memory_used_gb / total_mem_gb) * 100\n",
        "                    print(f\"Frame {frame_index}/{frame_count} - GPU Memory: {memory_used_gb:.2f}GB / {total_mem_gb:.2f}GB ({usage_percent:.1f}%)\")\n",
        "\n",
        "            # Clean GPU memory every 50 frames\n",
        "            if frame_index % 50 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        # Ensure 100% progress at the end\n",
        "        if progress is not None:\n",
        "            progress(1.0)\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        print(\"Processing complete!\")\n",
        "\n",
        "        # Now combine the processed video with the original audio\n",
        "        if has_audio:\n",
        "            print(\"Combining video with original audio...\")\n",
        "            if combine_video_audio(temp_video_path, temp_audio_path, output_path):\n",
        "                print(\"Successfully combined video with audio\")\n",
        "            else:\n",
        "                print(\"Audio combination failed. Using high quality encoding for video-only output...\")\n",
        "                # Fall back to just processing the video without audio\n",
        "                if torch.cuda.is_available():\n",
        "                    subprocess.run([\n",
        "                        'ffmpeg',\n",
        "                        '-i', temp_video_path,\n",
        "                        '-c:v', 'h264_nvenc',  # NVIDIA hardware encoding\n",
        "                        '-preset', 'p2',       # Medium quality/speed\n",
        "                        '-b:v', '8M',          # Bitrate\n",
        "                        '-y',                  # Overwrite output if exists\n",
        "                        output_path\n",
        "                    ], check=True, timeout=600)\n",
        "                else:\n",
        "                    subprocess.run([\n",
        "                        'ffmpeg',\n",
        "                        '-i', temp_video_path,\n",
        "                        '-c:v', 'libx264',     # CPU encoding\n",
        "                        '-preset', 'medium',   # Medium quality/speed\n",
        "                        '-crf', '23',          # Quality level\n",
        "                        '-y',                  # Overwrite output if exists\n",
        "                        output_path\n",
        "                    ], check=True, timeout=600)\n",
        "        else:\n",
        "            # No audio to add, just convert the video\n",
        "            print(\"No audio to add. Finalizing video with high quality encoding...\")\n",
        "            if torch.cuda.is_available():\n",
        "                subprocess.run([\n",
        "                    'ffmpeg',\n",
        "                    '-i', temp_video_path,\n",
        "                    '-c:v', 'h264_nvenc',  # NVIDIA hardware encoding\n",
        "                    '-preset', 'p2',       # Medium quality/speed\n",
        "                    '-b:v', '8M',          # Bitrate\n",
        "                    '-y',                  # Overwrite output if exists\n",
        "                    output_path\n",
        "                ], check=True, timeout=600)\n",
        "            else:\n",
        "                subprocess.run([\n",
        "                    'ffmpeg',\n",
        "                    '-i', temp_video_path,\n",
        "                    '-c:v', 'libx264',     # CPU encoding\n",
        "                    '-preset', 'medium',   # Medium quality/speed\n",
        "                    '-crf', '23',          # Quality level\n",
        "                    '-y',                  # Overwrite output if exists\n",
        "                    output_path\n",
        "                ], check=True, timeout=600)\n",
        "\n",
        "        # Clean up GPU memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Clean up segment file if we created one\n",
        "        if segment_path and os.path.exists(segment_path) and segment_path != input_path:\n",
        "            try:\n",
        "                os.remove(segment_path)\n",
        "                print(f\"Cleaned up temporary segment file: {segment_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not remove temporary segment file: {str(e)}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in process_video_to_3d_sbs: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generate_preview_frame(input_path, depth_intensity, convergence, eye_separation, frame_position=0.5):\n",
        "    \"\"\"Generate a preview frame for the given parameters\"\"\"\n",
        "    try:\n",
        "        # Validate input video\n",
        "        valid, result = validate_video(input_path)\n",
        "        if not valid:\n",
        "            raise ValueError(result)\n",
        "\n",
        "        # Setup MiDaS model\n",
        "        model, transform, device = setup_midas()\n",
        "\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "        # Get frame count and set position\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        target_frame = int(frame_count * frame_position)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n",
        "\n",
        "        # Read frame\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            cap.release()\n",
        "            raise ValueError(\"Could not read frame from video\")\n",
        "\n",
        "        # Estimate depth\n",
        "        depth_map = estimate_depth(frame, model, transform, device)\n",
        "\n",
        "        # Generate stereo views\n",
        "        left_view, right_view = generate_stereo_views(frame, depth_map, depth_intensity, convergence, eye_separation)\n",
        "\n",
        "        # Create side-by-side frame\n",
        "        sbs_frame_full = create_side_by_side(left_view, right_view)\n",
        "\n",
        "        # Create preview (smaller version)\n",
        "        preview_height = 360\n",
        "        preview_width = int(1920 * (preview_height / 1080))\n",
        "        sbs_frame = cv2.resize(sbs_frame_full, (preview_width, preview_height))\n",
        "\n",
        "        # Create comparison view with original frame\n",
        "        h, w = frame.shape[:2]\n",
        "        original_resized = cv2.resize(frame, (int(w * preview_height / h), preview_height))\n",
        "\n",
        "        # Create final preview\n",
        "        preview_width_total = original_resized.shape[1] + sbs_frame.shape[1] + 10\n",
        "        preview = np.zeros((preview_height, preview_width_total, 3), dtype=np.uint8)\n",
        "\n",
        "        # Add original frame\n",
        "        preview[:, :original_resized.shape[1]] = original_resized\n",
        "        # Add SBS frame\n",
        "        preview[:, original_resized.shape[1]+10:] = sbs_frame\n",
        "\n",
        "        # Add labels\n",
        "        cv2.putText(preview, \"Original\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(preview, \"3D SBS (16:9)\", (original_resized.shape[1]+20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "\n",
        "        # Convert to RGB for display\n",
        "        preview_rgb = cv2.cvtColor(preview, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Clean up GPU memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        return preview_rgb\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_preview_frame: {str(e)}\")\n",
        "        raise"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gradio_interface"
      },
      "source": [
        "## Gradio Interface\n",
        "\n",
        "Create an intuitive user interface for the 2D to 3D conversion with parameter controls and real-time preview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradio_app",
        "outputId": "b613aa7c-4f49-41b1-a916-5dd7e22ba478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create and launch the Gradio interface for 3D SBS conversion with audio support\"\"\"\n",
        "    # Global variables for state management\n",
        "    input_video_path = None\n",
        "    output_video_path = None\n",
        "    video_duration = 0  # Store video duration for segment selection\n",
        "\n",
        "    def save_uploaded_file(file_obj):\n",
        "        \"\"\"Helper function to save an uploaded file to disk\"\"\"\n",
        "        temp_dir = \"temp_videos\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Generate a filename with timestamp to avoid conflicts\n",
        "        timestamp = int(time.time())\n",
        "        file_name = f\"uploaded_video_{timestamp}.mp4\"\n",
        "        file_path = os.path.join(temp_dir, file_name)\n",
        "\n",
        "        print(f\"Saving uploaded file to {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Handle different file object types based on Gradio version\n",
        "            if isinstance(file_obj, str):\n",
        "                # It's a file path string, just copy the file\n",
        "                shutil.copy(file_obj, file_path)\n",
        "            elif hasattr(file_obj, 'name') and os.path.exists(file_obj.name):\n",
        "                # It's an object with a name attribute that points to a real file\n",
        "                shutil.copy(file_obj.name, file_path)\n",
        "            else:\n",
        "                # Try multiple approaches based on different versions of Gradio\n",
        "                if hasattr(file_obj, 'read') and callable(file_obj.read):\n",
        "                    # It's a file-like object, read and write it\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        f.write(file_obj.read())\n",
        "                elif hasattr(file_obj, '_path') and os.path.exists(file_obj._path):\n",
        "                    # Some versions of Gradio use a _path attribute\n",
        "                    shutil.copy(file_obj._path, file_path)\n",
        "                else:\n",
        "                    # Fall back to trying to directly access file object (may not work in all cases)\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        if isinstance(file_obj, bytes):\n",
        "                            f.write(file_obj)\n",
        "                        else:\n",
        "                            f.write(str(file_obj).encode('utf-8'))\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving file: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "        # Ensure the video is in H.264 format\n",
        "        return ensure_h264_mp4(file_path, temp_dir)\n",
        "\n",
        "    def upload_video(video_file):\n",
        "        \"\"\"Handle video upload\"\"\"\n",
        "        nonlocal input_video_path, video_duration\n",
        "\n",
        "        if video_file is None:\n",
        "            return None, \"Please upload a video file\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "        try:\n",
        "            # Save the uploaded file to disk and ensure H.264 encoding\n",
        "            input_video_path = save_uploaded_file(video_file)\n",
        "\n",
        "            # Validate video\n",
        "            valid, result = validate_video(input_video_path)\n",
        "            if not valid:\n",
        "                return None, result, gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "            # Get video duration for segment selection\n",
        "            video_duration = result.get(\"duration_sec\", 0)\n",
        "            if video_duration <= 0:\n",
        "                video_duration = get_video_duration(input_video_path)\n",
        "\n",
        "            # Update segment sliders\n",
        "            start_slider = gr.Slider(minimum=0, maximum=video_duration, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "            end_slider = gr.Slider(minimum=0, maximum=video_duration, value=video_duration, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Generate a preview frame\n",
        "            preview = generate_preview_frame(input_video_path, 0.5, 5.0, 2.5)\n",
        "\n",
        "            # Enable segment checkbox only if video is longer than 30 seconds\n",
        "            enable_segment = video_duration > 30\n",
        "\n",
        "            return preview, f\"Video loaded successfully: {result['width']}x{result['height']}, {result['fps']:.2f} FPS, {result['frame_count']} frames, {result['size_mb']:.2f}MB, Duration: {video_duration:.2f}s\", start_slider, end_slider, enable_segment\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing video upload: {str(e)}\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "    def download_from_url_handler(url):\n",
        "        \"\"\"Handle video URL input\"\"\"\n",
        "        nonlocal input_video_path, video_duration\n",
        "\n",
        "        if not url or url.strip() == \"\":\n",
        "            return None, \"Please enter a valid URL\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "        try:\n",
        "            # Download video and convert to H.264 if needed\n",
        "            input_video_path = download_from_url(url)\n",
        "\n",
        "            # Validate video\n",
        "            valid, result = validate_video(input_video_path)\n",
        "            if not valid:\n",
        "                return None, result, gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "            # Get video duration for segment selection\n",
        "            video_duration = result.get(\"duration_sec\", 0)\n",
        "            if video_duration <= 0:\n",
        "                video_duration = get_video_duration(input_video_path)\n",
        "\n",
        "            # Update segment sliders\n",
        "            start_slider = gr.Slider(minimum=0, maximum=video_duration, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "            end_slider = gr.Slider(minimum=0, maximum=video_duration, value=video_duration, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Generate a preview frame\n",
        "            preview = generate_preview_frame(input_video_path, 0.5, 5.0, 2.5)\n",
        "\n",
        "            # Enable segment checkbox only if video is longer than 30 seconds\n",
        "            enable_segment = video_duration > 30\n",
        "\n",
        "            return preview, f\"Video downloaded and converted successfully: {result['width']}x{result['height']}, {result['fps']:.2f} FPS, {result['frame_count']} frames, {result['size_mb']:.2f}MB, Duration: {video_duration:.2f}s\", start_slider, end_slider, enable_segment\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error downloading video: {str(e)}\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "    def update_preview(depth_intensity, convergence, eye_separation):\n",
        "        \"\"\"Update preview based on parameter changes\"\"\"\n",
        "        nonlocal input_video_path\n",
        "\n",
        "        if input_video_path is None or not os.path.exists(input_video_path):\n",
        "            return None, \"No video loaded\"\n",
        "\n",
        "        try:\n",
        "            # Generate new preview with current parameters\n",
        "            preview = generate_preview_frame(input_video_path, depth_intensity, convergence, eye_separation)\n",
        "            return preview, \"Preview updated with new parameters\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Error updating preview: {str(e)}\"\n",
        "\n",
        "    def update_end_time(start_time):\n",
        "        \"\"\"Update the end time slider to ensure it's always greater than start time\"\"\"\n",
        "        return gr.Slider(minimum=start_time + 0.1, maximum=video_duration, value=max(start_time + 0.1, video_duration))\n",
        "\n",
        "    def sync_segment_values(use_segment, segment_start, segment_end):\n",
        "        \"\"\"Synchronize segment values between tabs\"\"\"\n",
        "        # For Gradio compatibility, return a tuple of values instead of a dictionary\n",
        "        return use_segment, segment_start, segment_end\n",
        "\n",
        "    def process_video(depth_intensity, convergence, eye_separation, use_segment, segment_start, segment_end, progress=gr.Progress()):\n",
        "        \"\"\"Process the video with the given parameters\"\"\"\n",
        "        nonlocal input_video_path, output_video_path, video_duration\n",
        "\n",
        "        if input_video_path is None or not os.path.exists(input_video_path):\n",
        "            return None, \"No video loaded\"\n",
        "\n",
        "        try:\n",
        "            # Create output directory\n",
        "            output_dir = \"output_videos\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Generate output filename\n",
        "            base_name = os.path.basename(input_video_path)\n",
        "            name, ext = os.path.splitext(base_name)\n",
        "\n",
        "            # Add segment info to output filename if using segment\n",
        "            if use_segment and segment_start is not None and segment_end is not None and segment_start < segment_end:\n",
        "                output_video_path = os.path.join(output_dir, f\"{name}_3D_SBS_{int(segment_start)}-{int(segment_end)}s.mp4\")\n",
        "            else:\n",
        "                output_video_path = os.path.join(output_dir, f\"{name}_3D_SBS.mp4\")\n",
        "\n",
        "            # Process video\n",
        "            process_video_to_3d_sbs(\n",
        "                input_path=input_video_path,\n",
        "                output_path=output_video_path,\n",
        "                depth_intensity=depth_intensity,\n",
        "                convergence=convergence,\n",
        "                eye_separation=eye_separation,\n",
        "                progress=progress,\n",
        "                use_segment=use_segment,\n",
        "                segment_start=segment_start if use_segment else None,\n",
        "                segment_end=segment_end if use_segment else None\n",
        "            )\n",
        "\n",
        "            segment_info = f\" (segment {segment_start:.1f}s-{segment_end:.1f}s)\" if use_segment else \"\"\n",
        "            return output_video_path, f\"Video processed successfully{segment_info}. Saved to {output_video_path} with 16:9 aspect ratio (1920x1080) as requested.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing video: {str(e)}\"\n",
        "\n",
        "    # Create the Gradio interface\n",
        "    with gr.Blocks(title=\"2D to 3D SBS Video Converter (GPU Optimized)\") as app:\n",
        "        gr.Markdown(\"# 2D to 3D Side-by-Side Video Converter (GPU Optimized)\")\n",
        "        gr.Markdown(\"Convert standard 2D videos to stereoscopic 3D SBS format for VR viewing. Output has a 16:9 aspect ratio (1920x1080) with both eye views side by side. **Preserves original audio track** in the output video.\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_info = f\"Using GPU: {torch.cuda.get_device_name(0)} with {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB memory\"\n",
        "            gr.Markdown(f\"**{gpu_info}**\")\n",
        "        else:\n",
        "            gr.Markdown(\"**Running in CPU mode. Processing will be slower without GPU acceleration.**\")\n",
        "\n",
        "        with gr.Tab(\"Upload Video\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Video upload widget\n",
        "                    upload_input = gr.File(\n",
        "                        label=\"Upload Video File (max 500MB)\",\n",
        "                        file_types=[\"video\"],\n",
        "                        file_count=\"single\"\n",
        "                    )\n",
        "                    upload_button = gr.Button(\"Upload and Preview\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    # Preview and status\n",
        "                    preview = gr.Image(label=\"Preview\")\n",
        "                    status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    # Segment selection (initially hidden/disabled)\n",
        "                    use_segment = gr.Checkbox(label=\"Process a specific segment of the video\", value=False, interactive=False)\n",
        "                    segment_start = gr.Slider(minimum=0, maximum=1, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "                    segment_end = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Connect upload button\n",
        "            upload_button.click(\n",
        "                upload_video,\n",
        "                inputs=[upload_input],\n",
        "                outputs=[preview, status, segment_start, segment_end, use_segment]\n",
        "            )\n",
        "\n",
        "            # Update end time slider when start time changes to maintain valid range\n",
        "            segment_start.change(update_end_time, inputs=[segment_start], outputs=[segment_end])\n",
        "\n",
        "        with gr.Tab(\"Video URL\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # URL input widget\n",
        "                    url_input = gr.Textbox(label=\"Video URL (YouTube or direct link)\")\n",
        "                    url_button = gr.Button(\"Download and Preview\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    # Preview and status (shared with upload tab)\n",
        "                    url_preview = gr.Image(label=\"Preview\")\n",
        "                    url_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    # Segment selection (initially hidden/disabled)\n",
        "                    url_use_segment = gr.Checkbox(label=\"Process a specific segment of the video\", value=False, interactive=False)\n",
        "                    url_segment_start = gr.Slider(minimum=0, maximum=1, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "                    url_segment_end = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Connect URL button\n",
        "            url_button.click(\n",
        "                download_from_url_handler,\n",
        "                inputs=[url_input],\n",
        "                outputs=[url_preview, url_status, url_segment_start, url_segment_end, url_use_segment]\n",
        "            )\n",
        "\n",
        "            # Update end time slider when start time changes to maintain valid range\n",
        "            url_segment_start.change(update_end_time, inputs=[url_segment_start], outputs=[url_segment_end])\n",
        "\n",
        "        with gr.Tab(\"Convert to 3D\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Depth control parameters\n",
        "                    depth_intensity = gr.Slider(\n",
        "                        minimum=0.0, maximum=1.0, value=0.5, step=0.01,\n",
        "                        label=\"Depth Intensity\",\n",
        "                        info=\"Controls the strength of the 3D effect (0.0-1.0)\"\n",
        "                    )\n",
        "\n",
        "                    convergence = gr.Slider(\n",
        "                        minimum=1.0, maximum=10.0, value=5.0, step=0.1,\n",
        "                        label=\"Convergence Distance\",\n",
        "                        info=\"Adjusts the perceived distance of objects (1.0-10.0)\"\n",
        "                    )\n",
        "\n",
        "                    eye_separation = gr.Slider(\n",
        "                        minimum=0.1, maximum=5.0, value=2.5, step=0.1,\n",
        "                        label=\"Eye Separation\",\n",
        "                        info=\"Controls the distance between virtual cameras (0.1-5.0)\"\n",
        "                    )\n",
        "\n",
        "                    # Segment selection (duplicated for this tab for better UX)\n",
        "                    conv_use_segment = gr.Checkbox(label=\"Process a specific segment of the video\", value=False)\n",
        "                    conv_segment_start = gr.Slider(minimum=0, maximum=video_duration, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "                    conv_segment_end = gr.Slider(minimum=0, maximum=video_duration, value=video_duration, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "                    # Update end time slider when start time changes\n",
        "                    conv_segment_start.change(update_end_time, inputs=[conv_segment_start], outputs=[conv_segment_end])\n",
        "\n",
        "                    # Update preview button\n",
        "                    update_button = gr.Button(\"Update Preview\")\n",
        "\n",
        "                    # Process button\n",
        "                    process_button = gr.Button(\"Process Video\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    # Preview and status (shared)\n",
        "                    convert_preview = gr.Image(label=\"Preview\")\n",
        "                    convert_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    # Output video\n",
        "                    output_video = gr.Video(label=\"Converted 3D SBS Video (16:9 aspect ratio with audio)\")\n",
        "\n",
        "            # Connect update preview button\n",
        "            update_button.click(\n",
        "                update_preview,\n",
        "                inputs=[depth_intensity, convergence, eye_separation],\n",
        "                outputs=[convert_preview, convert_status]\n",
        "            )\n",
        "\n",
        "            # Connect process button\n",
        "            process_button.click(\n",
        "                process_video,\n",
        "                inputs=[depth_intensity, convergence, eye_separation, conv_use_segment, conv_segment_start, conv_segment_end],\n",
        "                outputs=[output_video, convert_status]\n",
        "            )\n",
        "\n",
        "            # Synchronize segment values between tabs\n",
        "            # Connect the segment controls to the sync function\n",
        "            use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[use_segment, segment_start, segment_end],\n",
        "                outputs=[conv_use_segment, conv_segment_start, conv_segment_end]\n",
        "            )\n",
        "\n",
        "            url_use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[url_use_segment, url_segment_start, url_segment_end],\n",
        "                outputs=[conv_use_segment, conv_segment_start, conv_segment_end]\n",
        "            )\n",
        "\n",
        "            # Sync back from Convert tab to others\n",
        "            conv_use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[conv_use_segment, conv_segment_start, conv_segment_end],\n",
        "                outputs=[use_segment, segment_start, segment_end]\n",
        "            )\n",
        "\n",
        "            conv_use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[conv_use_segment, conv_segment_start, conv_segment_end],\n",
        "                outputs=[url_use_segment, url_segment_start, url_segment_end]\n",
        "            )\n",
        "\n",
        "        # Help tab\n",
        "        with gr.Tab(\"Help\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## How to Use This Tool\n",
        "\n",
        "            1. Upload a video file or provide a URL to a video (supports YouTube).\n",
        "            2. For longer videos, you can choose to process only a specific segment to save time and memory:\n",
        "               - Check the \"Process a specific segment\" box\n",
        "               - Set the start and end times in seconds\n",
        "            3. Adjust the depth parameters to control the 3D effect:\n",
        "               - **Depth Intensity**: Controls the strength of the 3D effect. Higher values create more pronounced depth.\n",
        "               - **Convergence Distance**: Adjusts where objects appear to be in relation to the screen plane.\n",
        "               - **Eye Separation**: Controls the virtual camera separation. Higher values create more extreme 3D effects.\n",
        "            4. Click \"Update Preview\" to see how your settings affect the 3D output.\n",
        "            5. Click \"Process Video\" to convert the entire video (or selected segment) to 3D SBS format.\n",
        "            6. Download the converted video for viewing in a VR headset or 3D display.\n",
        "\n",
        "            ## Video Segmentation\n",
        "\n",
        "            The video segment feature allows you to process only a portion of a longer video. This is useful for:\n",
        "            - Testing different 3D settings on a small clip before processing the entire video\n",
        "            - Processing very long videos in manageable chunks to avoid memory issues or timeouts\n",
        "            - Creating highlights in 3D from specific parts of a longer video\n",
        "\n",
        "            ## Output Format\n",
        "\n",
        "            - The final video will have a 16:9 aspect ratio (1920x1080)\n",
        "            - Each eye view is positioned side by side with appropriate proportions\n",
        "            - Black bars are added as needed to maintain the proper 16:9 aspect ratio\n",
        "            - H.264 encoded MP4 format for maximum compatibility\n",
        "            - Maintains the original video's frame rate\n",
        "\n",
        "            ## Supported Formats\n",
        "\n",
        "            - Input: MP4, AVI, MOV, WebM, MKV (up to 4K resolution, max 500MB)\n",
        "            - Output: H.264 encoded MP4 in Side-by-Side format (1920x1080)\n",
        "\n",
        "            ## Viewing the 3D Video\n",
        "\n",
        "            The output video is in Side-by-Side (SBS) format, which can be viewed in:\n",
        "            - VR headsets using video players that support SBS format\n",
        "            - 3D TVs with SBS viewing mode\n",
        "            - Special 3D viewers like Google Cardboard with SBS-compatible apps\n",
        "\n",
        "            ## GPU Optimization\n",
        "\n",
        "            This version of the converter is optimized to take advantage of NVIDIA GPUs for faster processing:\n",
        "\n",
        "            - Batch processing of multiple frames at once to maximize GPU utilization\n",
        "            - GPU-accelerated depth map generation\n",
        "            - Optimized memory management to handle larger videos\n",
        "            - Hardware-accelerated video encoding when available\n",
        "\n",
        "            ## Troubleshooting\n",
        "\n",
        "            - If processing fails, try using a smaller segment of the video.\n",
        "            - For best results, use videos with good lighting and clear objects.\n",
        "            - If the 3D effect is too strong or causes discomfort, lower the Depth Intensity and Eye Separation values.\n",
        "            - If you experience issues with YouTube downloads, try using a direct video URL instead.\n",
        "            - If you experience any issues, check the status messages for error details.\n",
        "            \"\"\")\n",
        "\n",
        "    # Launch the app\n",
        "    app.launch(debug=True, share=True)\n",
        "\n",
        "# Initialize and launch the Gradio application\n",
        "create_gradio_interface()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://49d51384e9fa8adbe0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://49d51384e9fa8adbe0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Google Drive URL. Downloading with gdown...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1t0YHqmC4ZImt8rp5BvqDkJcxzqHHUeaP\n",
            "From (redirected): https://drive.google.com/uc?id=1t0YHqmC4ZImt8rp5BvqDkJcxzqHHUeaP&confirm=t&uuid=a90d5c50-ed67-4e77-a304-c2e67644ca4a\n",
            "To: /content/temp_videos/downloaded_video_1751922101.mp4\n",
            "100%|| 4.13G/4.13G [00:51<00:00, 80.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded from Google Drive to temp_videos/downloaded_video_1751922101.mp4\n",
            "Checking encoding of temp_videos/downloaded_video_1751922101.mp4...\n",
            "Video is already H.264 encoded (codec: h264)\n",
            "Loading MiDaS depth estimation model...\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory Allocated: 0.00 GB\n",
            "Memory Reserved: 0.00 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /tmp/tmp1vli0p1b/master.zip\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\" to /tmp/tmp1vli0p1b/checkpoints/dpt_large_384.pt\n",
            "100%|| 1.28G/1.28G [00:13<00:00, 100MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping TorchScript optimization due to compatibility issues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /tmp/tmp1vli0p1b/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory After Model Load: 1.39 GB\n",
            "MiDaS model loaded successfully!\n",
            "No audio stream detected in the video\n",
            "Extracting segment from 0.00s to 1233.52s...\n",
            "Segment extracted successfully: temp_videos/segment_1751922285.mp4\n",
            "Using video segment from 0s to 1233.5249999999999s\n",
            "Loading MiDaS depth estimation model...\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory Allocated: 0.03 GB\n",
            "Memory Reserved: 0.05 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /tmp/tmprk0e9_nk/master.zip\n",
            "Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\" to /tmp/tmprk0e9_nk/checkpoints/dpt_large_384.pt\n",
            "100%|| 1.28G/1.28G [00:15<00:00, 90.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping TorchScript optimization due to compatibility issues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /tmp/tmprk0e9_nk/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory After Model Load: 1.40 GB\n",
            "MiDaS model loaded successfully!\n",
            "Using batch size: 1 for 3840x2160 video\n",
            "Available GPU memory: 14.43GB\n",
            "Initial GPU Memory: 1.40GB / 15.83GB\n",
            "Frame 10/73713 - GPU Memory: 1.40GB / 15.83GB (8.9%)\n",
            "Frame 20/73713 - GPU Memory: 1.40GB / 15.83GB (8.9%)\n",
            "Frame 30/73713 - GPU Memory: 1.40GB / 15.83GB (8.9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## Troubleshooting and Tips\n",
        "\n",
        "If you experience issues with the application, here are some tips and solutions:\n",
        "\n",
        "1. **GPU Memory Errors**: If you encounter CUDA out of memory errors:\n",
        "   - Use the segment feature to process smaller portions of the video\n",
        "   - Restart the runtime to clear memory\n",
        "   - Process a shorter or lower resolution video\n",
        "\n",
        "2. **Loading Time**: The MiDaS model takes time to download and load initially. Be patient during first use.\n",
        "\n",
        "3. **Quality Issues**: The quality of the 3D effect depends on the input video quality and the accuracy of the depth map. Videos with clear objects and good lighting work best.\n",
        "\n",
        "4. **Processing Speed**: Even with GPU acceleration, depth estimation is computationally intensive. Processing time depends on video length, resolution, and available GPU resources.\n",
        "\n",
        "5. **View Distance**: If objects appear too close or too far in the 3D output, adjust the Convergence Distance parameter.\n",
        "\n",
        "6. **Eye Strain**: If the 3D effect causes discomfort, reduce the Depth Intensity and Eye Separation values for a more comfortable viewing experience.\n",
        "\n",
        "7. **YouTube Downloads**: If YouTube downloads fail, try using a different web browser to copy a direct video URL.\n",
        "\n",
        "8. **Runtime Disconnections**: For long videos, Colab might disconnect. Use the segment feature to process the video in chunks.\n",
        "\n",
        "9. **Video Conversion**: If video conversion seems stuck, try restarting the notebook and using a smaller video segment.\n",
        "\n",
        "10. **Maximizing GPU Usage**: This version attempts to use the full capacity of your GPU. You can monitor GPU usage in Colab using the command: `!nvidia-smi` in a new cell.\n",
        "\n",
        "11. **Processing Segments**: For videos longer than a few minutes, it's recommended to process them in 1-3 minute segments to avoid Colab timeouts and memory issues.\n",
        "\n",
        "12. **Best Results**: For optimal 3D conversion, use videos with:\n",
        "   - Clear foreground and background separation\n",
        "   - Good lighting conditions\n",
        "   - Minimal fast camera movement\n",
        "   - Higher resolution (1080p or above)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2D_to_3D_SBS_Converter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}